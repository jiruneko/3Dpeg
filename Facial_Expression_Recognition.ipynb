{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiruneko/3Dpeg/blob/master/Facial_Expression_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vtm2E3sTWpw"
      },
      "outputs": [],
      "source": [
        "!pip install timm==0.6.7\n",
        "!pip install moviepy==0.2.3.5 imageio==2.4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho34Z6g_hB2t"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "\n",
        "!git clone https://github.com/HSE-asavchenko/face-emotion-recognition.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R45MKY0aiYfH"
      },
      "outputs": [],
      "source": [
        "%cd /content/face-emotion-recognition/src\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "from moviepy.editor import *\n",
        "from moviepy.video.fx.resize import resize\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "from facial_analysis import FacialImageProcessing\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('using device:', DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJI0VQaHjHPB"
      },
      "outputs": [],
      "source": [
        "%cd /content/face-emotion-recognition/src\n",
        "\n",
        "# インデックスと分類クラスの対応表(dict)\n",
        "idx_to_class = {\n",
        "    0: 'Anger',     # 怒り\n",
        "    1: 'Contempt',  # 軽蔑\n",
        "    2: 'Disgust',   # 嫌悪感\n",
        "    3: 'Fear',      # 恐れ\n",
        "    4: 'Happiness', # 幸福\n",
        "    5: 'Neutral',   # ニュートラル\n",
        "    6: 'Sadness',   # 悲しみ\n",
        "    7: 'Surprise'   # 驚き\n",
        "    }\n",
        "\n",
        "\n",
        "IMG_SIZE = 260\n",
        "MODEL_PATH = '/content/face-emotion-recognition/models/affectnet_emotions/enet_b2_8.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tR5awXbWjYd6"
      },
      "outputs": [],
      "source": [
        "imgProcessing = FacialImageProcessing(False)\n",
        "\n",
        "img_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((IMG_SIZE,IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "            )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHwwBueTjeZA"
      },
      "outputs": [],
      "source": [
        "model = torch.load(MODEL_PATH)\n",
        "model=model.to(DEVICE)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7u-Bp3wPjj5N"
      },
      "outputs": [],
      "source": [
        "%cd /content/face-emotion-recognition/src\n",
        "\n",
        "!wget -c https://www.pakutaso.com/shared/img/thumb/kuchikomi1134_TP_V4.jpg \\\n",
        "      -O ../test_images/angry.jpg\n",
        "\n",
        "!wget -c https://www.pakutaso.com/shared/img/thumb/AL003-ocyaitadakujyoushi20140722_TP_V4.jpg \\\n",
        "      -O ../test_images/two_persons.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i2kPHLljqgk"
      },
      "outputs": [],
      "source": [
        "def predict(input_file_path, text_size=None, print_time=False):\n",
        "  # 画像をOpenCV2でロード\n",
        "  frame_bgr = cv2.imread(input_file_path)\n",
        "  # BGR->RGB\n",
        "  frame = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "  # 顔検出\n",
        "  bounding_boxes, points = imgProcessing.detect_faces(frame)\n",
        "  # 描画文字サイズ算出\n",
        "  if text_size == None:\n",
        "    h, w, _ = frame.shape\n",
        "    text_size = w/800\n",
        "\n",
        "  points = points.T\n",
        "  for bbox,p in zip(bounding_boxes, points):\n",
        "    # 顔画像取得\n",
        "    box = bbox.astype(np.int64)\n",
        "    x1,y1,x2,y2 = box[0:4]    \n",
        "    face_img = frame[y1:y2,x1:x2,:]\n",
        "    \n",
        "    # 前処理\n",
        "    img_tensor = img_transforms(Image.fromarray(face_img))\n",
        "    img_tensor.unsqueeze_(0)\n",
        "\n",
        "    # Facial Expression Recognition\n",
        "    if print_time:\n",
        "      start = time.time()\n",
        "    scores = model(img_tensor.to(DEVICE))\n",
        "    if print_time:\n",
        "      end = time.time()\n",
        "      print('processing time: %2f ms'% ((end - start)*1000.))\n",
        "\n",
        "    # 予測結果描画\n",
        "    scores=scores[0].data.cpu().numpy()\n",
        "\n",
        "    # 検出した顔の矩形を描画\n",
        "    cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), thickness = 2)\n",
        "    # 表情認識結果を描画\n",
        "    text = idx_to_class[np.argmax(scores)]\n",
        "    # 視認性を上げるため白字で文字の外枠を囲む\n",
        "    cv2.putText(frame, text, (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, text_size, (255, 255, 255), thickness=9)\n",
        "    cv2.putText(frame, text, (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, text_size, (0, 0, 0), thickness=5)\n",
        "\n",
        "  return frame, scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbCIoOlZjzSl"
      },
      "outputs": [],
      "source": [
        "%cd /content/face-emotion-recognition/src\n",
        "\n",
        "input_imgs = glob.glob(\"../test_images/*.jpg\")\n",
        "\n",
        "for img_path in input_imgs:\n",
        "  frame, scores = predict(img_path, text_size=None, print_time=True)\n",
        "\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  plt.axis('off')\n",
        "  plt.imshow(frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiDu2gmKj260"
      },
      "outputs": [],
      "source": [
        "#@markdown 動画の切り抜き範囲(秒)を指定してください。\\\n",
        "#@markdown 30秒以上の場合OOM発生の可能性が高いため注意\n",
        "start_sec =  1#@param {type:\"integer\"}\n",
        "end_sec =  10#@param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAee-ztikEPV"
      },
      "outputs": [],
      "source": [
        "%cd /content/face-emotion-recognition\n",
        "!rm -rf test_video\n",
        "!mkdir -p test_video/frames\n",
        "!mkdir -p test_video/outputs\n",
        "%cd test_video\n",
        "\n",
        "# 動画アップロードは、このタイミングで下部のファイル選択から行い、100%になればアップロード完了\n",
        "uploaded = files.upload()\n",
        "uploaded = list(uploaded.keys())\n",
        "file_name = uploaded[0]\n",
        "\n",
        "upload_path = os.path.join(\"/content/face-emotion-recognition/test_video\", file_name)\n",
        "print(\"upload file here:\", upload_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1k8dCNDkSMo"
      },
      "outputs": [],
      "source": [
        "subclip_path = \"/content/face-emotion-recognition/test_video/subclip.mp4\"\n",
        "\n",
        "with VideoFileClip(upload_path) as video:\n",
        "    subclip = video.subclip(start_sec, end_sec)\n",
        "    subclip.write_videofile(subclip_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBqDFxTq95Fq"
      },
      "outputs": [],
      "source": [
        "!ffmpeg -i {subclip_path} frames/%06d.png\n",
        "\n",
        "frames = glob.glob(\"/content/face-emotion-recognition/test_video/frames/*.png\")\n",
        "print(\"num of frames:\", len(frames))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkaBIOcZ95ES"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "\n",
        "# フレーム画像すべてをFacial Expression Recognition\n",
        "for img_path in tqdm(frames):\n",
        "  frame, scores = predict(img_path, text_size=1.0)\n",
        "  save_path = os.path.join(\n",
        "      \"/content/face-emotion-recognition/test_video/outputs\",\n",
        "      os.path.basename(img_path) )\n",
        "\n",
        "  # RGB->BGR\n",
        "  cv2.imwrite(save_path, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "end = time.time()\n",
        "print('processing time: %2f sec'% ((end - start)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmQBsNE8-bKJ"
      },
      "outputs": [],
      "source": [
        "!ffmpeg -i \"/content/face-emotion-recognition/test_video/outputs/%06d.png\" \\\n",
        "        -c:v libx264 -vf \"format=yuv420p\" \"/content/face-emotion-recognition/test_video/outputs/result.mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "szEhqPHmAey9"
      },
      "outputs": [],
      "source": [
        "clip = VideoFileClip(\"/content/face-emotion-recognition/test_video/outputs/result.mp4\")\n",
        "clip = resize(clip, height=640)\n",
        "clip.ipython_display()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvQx9z-zBHqu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaT1vya8RBPHj74YgUM7xU",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}